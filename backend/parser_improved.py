"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Avito Parser - —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ –∏ –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import time
import re
import random
from loguru import logger
from models import Announcement
from database import db


class ImprovedAvitoParser:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, —Ä–æ—Ç–∞—Ü–∏–µ–π, –∞–Ω—Ç–∏–±–∞–Ω –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏"""
    
    def __init__(self, config: dict):
        self.config = config
        self.session = requests.Session()
        self.proxies = config.get('proxies', [])
        self.proxy_index = 0
        self._setup_session()
        
    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å User-Agent –∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏"""
        user_agent = self.config.get('parser', {}).get('user_agent') or \
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def _get_next_proxy(self) -> Optional[Dict]:
        """–†–æ—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏"""
        if not self.proxies:
            return None
        
        proxy = self.proxies[self.proxy_index % len(self.proxies)]
        self.proxy_index += 1
        
        return {'http': proxy, 'https': proxy}
    
    def parse_city(self, city_data: dict, max_pages: int = 3) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –≥–æ—Ä–æ–¥–∞"""
        city_name = city_data['name']
        url_slug = city_data['url_slug']
        announcements = []
        
        logger.info(f"üåç –ü–∞—Ä—Å–∏–Ω–≥ –≥–æ—Ä–æ–¥–∞: {city_name}")
        
        for source in city_data.get('sources', []):
            if not source.get('enabled', True):
                logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ (–æ—Ç–∫–ª—é—á–µ–Ω–∞): {source['category']}")
                continue
            
            url = f"https://www.avito.ru/{url_slug}/{source['url_path']}"
            logger.info(f"üîç {source['category']}: {url}")
            
            try:
                items = self.parse_listing_page(url, max_pages, source['category'], city_name)
                announcements.extend(items)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ (–∞–Ω—Ç–∏–±–∞–Ω)
                delay = self.config.get('parser', {}).get('delay_between_requests', 2)
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {source['category']}: {e}")
                continue
        
        logger.info(f"‚úÖ –ì–æ—Ä–æ–¥ {city_name}: –Ω–∞–π–¥–µ–Ω–æ {len(announcements)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        return announcements
    
    def parse_listing_page(self, url: str, max_pages: int = 3, category: str = "general", city: str = "") -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–∞–Ω–∞"""
        announcements = []
        
        for page in range(1, max_pages + 1):
            page_url = f"{url}?p={page}" if page > 1 else url
            
            try:
                # –†–æ—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏ –∏ User-Agent
                proxies = self._get_next_proxy()
                
                logger.debug(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{max_pages}: {page_url}")
                
                response = self.session.get(
                    page_url,
                    timeout=self.config.get('parser', {}).get('timeout', 30),
                    proxies=proxies,
                    allow_redirects=True
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò—â–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è (—Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤—ë—Ä—Å—Ç–∫–∏ –ê–≤–∏—Ç–æ)
                items = soup.find_all('div', {'data-marker': 'item'})
                
                if not items:
                    logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–≤–æ–∑–º–æ–∂–Ω–æ, –ê–≤–∏—Ç–æ –∏–∑–º–µ–Ω–∏–ª–∞ –≤—ë—Ä—Å—Ç–∫—É)")
                    break
                
                for item in items:
                    try:
                        announcement = self._parse_item(item, category, city)
                        if announcement:
                            announcements.append(announcement)
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                        continue
                
                # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ (–∞–Ω—Ç–∏–±–∞–Ω)
                delay = random.uniform(1, 3)
                time.sleep(delay)
                
            except requests.exceptions.ProxyError:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}, –ø—Ä–æ–±—É—é –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
                try:
                    response = self.session.get(page_url, timeout=30)
                    response.raise_for_status()
                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –±–µ–∑ –ø—Ä–æ–∫—Å–∏
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –±–µ–∑ –ø—Ä–æ–∫—Å–∏: {e}")
                    break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                break
        
        return announcements
    
    def _parse_item(self, item, category: str = "", city: str = "") -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        try:
            # ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            avito_id = item.get('data-item-id')
            if not avito_id:
                return None
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = item.find('a', {'itemprop': 'url'}) or item.find('a', {'data-marker': 'item-title'})
            title = title_elem.get_text(strip=True) if title_elem else None
            
            if not title:
                return None
            
            # URL
            url = title_elem.get('href') if title_elem else None
            if url and not url.startswith('http'):
                url = f"https://www.avito.ru{url}"
            
            # –¶–µ–Ω–∞ (–ª—É—á—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            price = self._extract_price(item)
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc_elem = item.find('div', {'class': re.compile('.*description.*', re.I)})
            description = desc_elem.get_text(strip=True) if desc_elem else ""
            
            # –ö–∞—Ä—Ç–∏–Ω–∫–∞
            img_elem = item.find('img', {'itemprop': 'image'}) or item.find('img')
            image_url = img_elem.get('src') if img_elem else None
            
            # –õ–æ–∫–∞—Ü–∏—è
            location_elem = item.find('div', {'class': re.compile('.*geo.*', re.I)})
            location = location_elem.get_text(strip=True) if location_elem else city
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–≤—Ç–æ—Ä–∞ (private –∏–ª–∏ business)
            author_type = self._detect_author_type(item)
            
            return {
                'avito_id': avito_id,
                'title': title,
                'description': description,
                'price': price,
                'url': url,
                'image_urls': [image_url] if image_url else [],
                'location': location,
                'author_type': author_type,
                'category': category,
            }
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            return None
    
    def _extract_price(self, item) -> Optional[float]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            price_elem = item.find('meta', {'itemprop': 'price'})
            if price_elem:
                price_text = price_elem.get('content')
            else:
                price_elem = item.find('span', {'data-marker': 'item-price'})
                if price_elem:
                    price_text = price_elem.get_text()
                else:
                    # –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
                    price_elem = item.find('span', {'class': re.compile('.*price.*', re.I)})
                    price_text = price_elem.get_text() if price_elem else None
            
            if not price_text:
                return None
            
            # –û—á–∏—â–∞–µ–º –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
            price_clean = re.sub(r'[^\d]', '', str(price_text))
            return float(price_clean) if price_clean else None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–µ–Ω—ã: {e}")
            return None
    
    def _detect_author_type(self, item) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∞–≤—Ç–æ—Ä–∞ (private –∏–ª–∏ business)"""
        try:
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –±–∏–∑–Ω–µ—Å–∞
            business_indicators = [
                'data-marker.*shop',
                'data-marker.*company',
                'class.*shop',
                'class.*company',
                'class.*seller',
                'class.*business',
            ]
            
            item_str = str(item)
            for indicator in business_indicators:
                if re.search(indicator, item_str, re.I):
                    return "business"
            
            return "private"
        except:
            return "private"
    
    def filter_announcements(self, announcements: List[Dict], stop_words: List[str]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        filtered = []
        
        for ann in announcements:
            # 1. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∏–∑–Ω–µ—Å
            if ann.get('author_type') == 'business':
                logger.debug(f"–§–∏–ª—å—Ç—Ä: –±–∏–∑–Ω–µ—Å - {ann['title']}")
                continue
            
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ)
            text_to_check = f"{ann.get('title', '')} {ann.get('description', '')}".lower()
            found_stop_word = False
            for word in stop_words:
                if word.lower() in text_to_check:
                    logger.debug(f"–§–∏–ª—å—Ç—Ä: —Å—Ç–æ–ø-—Å–ª–æ–≤–æ '{word}' - {ann['title']}")
                    found_stop_word = True
                    break
            
            if found_stop_word:
                continue
            
            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É (–∏—Å–∫–ª—é—á–∞–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω–æ–µ)
            if ann.get('price') is None or ann.get('price') == 0:
                logger.debug(f"–§–∏–ª—å—Ç—Ä: –Ω–µ—Ç —Ü–µ–Ω—ã - {ann['title']}")
                continue
            
            # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ø–∞–º–∞)
            if len(ann.get('description', '')) < 10:
                logger.debug(f"–§–∏–ª—å—Ç—Ä: –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ - {ann['title']}")
                continue
            
            filtered.append(ann)
        
        logger.info(f"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered)}/{len(announcements)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        return filtered
    
    def save_to_db(self, announcements: List[Dict]) -> Dict[str, int]:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"""
        stats = {'new': 0, 'duplicate': 0, 'updated': 0}
        session = db.get_session()
        
        try:
            for ann_data in announcements:
                avito_id = ann_data['avito_id']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏
                existing = session.query(Announcement).filter_by(avito_id=avito_id).first()
                
                if existing:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ —Ü–µ–Ω–∞
                    new_price = ann_data.get('price')
                    if new_price and existing.price != new_price:
                        existing.price = new_price
                        existing.last_price = existing.price
                        existing.status = 'updated'
                        stats['updated'] += 1
                        logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Ü–µ–Ω–∞: {existing.title}")
                    else:
                        stats['duplicate'] += 1
                else:
                    # –ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    announcement = Announcement(
                        avito_id=avito_id,
                        title=ann_data.get('title'),
                        description=ann_data.get('description'),
                        price=ann_data.get('price'),
                        category=ann_data.get('category'),
                        url=ann_data.get('url'),
                        image_urls=ann_data.get('image_urls'),
                        author_type=ann_data.get('author_type'),
                        location=ann_data.get('location'),
                        content_hash=Announcement.generate_hash(
                            avito_id,
                            ann_data.get('title', ''),
                            ann_data.get('description', '')
                        ),
                        status='new'
                    )
                    session.add(announcement)
                    stats['new'] += 1
                    logger.info(f"‚ú® –ù–æ–≤–æ–µ: {announcement.title}")
            
            session.commit()
            
        except Exception as e:
            session.rollback()
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
        finally:
            session.close()
        
        return stats
